# -*- coding: utf-8 -*-
"""notebook1e515c1542

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook1e515c1542-4254c512-d92f-4ab5-8638-7b81a95743ce.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240428/auto/storage/goog4_request%26X-Goog-Date%3D20240428T184321Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D436f57af713625dfe029629ac93ba0a07a392256b598c429f3218625a98a4c1b425012bc7ddfc5fe260e7c4f92b636265420414b34337eabde13d6004d9f7387ab3e964c8fee52d43eb28930373026ebca98f48b897339d5d86d2180e32ea4ed1e75d55bc9f5d2add58c649fdd10f111a000931533af058d44984693efb0631789d029e5e1cfddd2f0d0acf22b7a5c0a4a668fdc2047b0781fd806adc2b6b5191d0a5b0ef786b8cd36cdcfc0ae0c47551fbb93490bfac777cf21f6c82f9b4058ef64a30a1ccd3e37f218f4c18de826b769eb6b4fb412e858351cc2eadb4d889aa416b73af89fe2d0ab25cb84fdcb331d9b675b44c08122d68cef5ca78c64c89f
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'air-quality-data-set:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1057064%2F1777920%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240428%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240428T184321Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D47ee3a9a74cf93bb7c6d2b7b5fe958529c5e8c8b2ab2d1e1ece2425cefc7a1c28b00f398fbfdb57b5b714a95af38c067e68e4eb4a14f3ec7dcc72eac3f486cce1d1d0db387f8b4c1fb43ecf2de6b76b2178c4274a828427cfab5ebf62ac2583087dd8e9f9f52115602f62688db2294116e97dd998ae28e37d117a8a99ba20d5bc3d44a700c84c63512f56276cf4a40e148effef735f0f6a1e3af7ce476140406d3b0eb939207af6b151b6371721a4b0a65529abb9d8af1407af34234c0296e7cf653c40f8bc1924563753b4c6aea8762d106460639eeb405c948fdafc58c0487452dc787f7cafc1bb39064d72bf48c743c389dc132527ce7b5293f29a9b43eab'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

#Importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Loading csv file on a dataframe

ar = pd.read_csv('../input/air-quality-data-set/AirQuality.csv',sep=';')
ar.head()

ar.info()

#Dropping CO(GT) and Unnamed columns
ar.drop(['CO(GT)','Unnamed: 15','Unnamed: 16'],axis = 1,inplace = True)

#Formatting some object columns from strings to floats

ar.replace(to_replace=',',value='.',regex=True,inplace=True)

for i in 'C6H6(GT) T RH AH'.split():
    ar[i] = pd.to_numeric(ar[i],errors='coerce')

#Replacing null data from -200 to NaN for posterior treatment

ar.replace(to_replace=-200,value=np.nan,inplace=True)

ar.info()

#Formatting Date and Time to datetime type

ar['Date'] = pd.to_datetime(ar['Date'],dayfirst=True)

ar['Time'] = pd.to_datetime(ar['Time'],format= '%H.%M.%S' ).dt.time

ar.head()

NMHC_ratio = ar['NMHC(GT)'].isna().sum()/len(ar['NMHC(GT)'])

print('The NMHC(GT) sensor has {:.2f}% of missing data.'.format(NMHC_ratio*100))

#Removing NMHC(GT) sensor due to amount of null values

ar.drop('NMHC(GT)', axis=1, inplace=True)

ar.info()

sns.set_theme(style="whitegrid")

for i in ar.columns[2:13]:
    sns.boxplot(x=ar[i])
    plt.title('Boxplot of the sensors data')
    plt.show()

# Removing Outliers with the Interquartile Range Method (IQR)

# Selecting only numerical columns for calculating quartiles
numerical_columns = ar.select_dtypes(include=[np.number]).columns

Q1 = ar[numerical_columns].quantile(0.25)  # first 25% of the data
Q3 = ar[numerical_columns].quantile(0.75)  # first 75% of the data
IQR = Q3 - Q1  # IQR = InterQuartile Range

scale = 2  # For Normal Distributions, scale = 1.5
lower_lim = Q1 - scale * IQR
upper_lim = Q3 + scale * IQR

lower_outliers = (ar[numerical_columns] < lower_lim)
upper_outliers = (ar[numerical_columns] > upper_lim)

#Checking the resulting outliers calculated by the above method (represented below as non-null values)

ar[ar.columns[2:13]][(lower_outliers | upper_outliers)].info()

#Create new DataFrame without the outliers

num_cols = list(ar.columns[2:13])
ar_out_IQR = ar[~((ar[num_cols] < (Q1 - 2 * IQR)) |(ar[num_cols] > (Q3 + 2 * IQR))).any(axis=1)]
ar_out_IQR.info()

#Removing NOx(GT) and NO2(GT) sensor data due the ammount of null values if compared to other sensors

pd.options.mode.chained_assignment = None
ar_out_IQR.drop(['NOx(GT)','NO2(GT)'],axis=1, inplace=True)
ar_out_IQR.info()

#Eliminating rows with NaN values

ar_filt = ar_out_IQR.dropna(how='any', axis=0)
ar_filt.reset_index(drop=True,inplace=True)

ar_filt.info()

#Adding a column with the week days

ar_filt['Week Day'] = ar_filt['Date'].dt.day_name()

#Rearranging columns

cols = ar_filt.columns.tolist()
cols = cols[:1] + cols[-1:] + cols[1:11]
ar_filt = ar_filt[cols]
ar_filt.head(10)

#Creating new dataframe with only wednesday data

ar_wed = ar_filt[ar_filt['Week Day'] == 'Wednesday']

#Plotting the mean hourly value of CO on Wednesdays

sns.barplot(x='Time',y='PT08.S1(CO)', data=ar_wed.sort_values('Time'))
plt.title('Mean Hourly Values of CO on Wednesdays')
plt.xticks(rotation=90)
plt.show()

#The peak concentration of CO in the city are between 8 AM and 9 AM and between 6 PM and 8 PM,
#beginnings and endings of office hours, respectively.

# Plotting correlation matrix

# Selecting only numerical columns for calculating correlation matrix
numerical_columns = ar.select_dtypes(include=[np.number]).columns

sns.heatmap(ar[numerical_columns].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

sns.pairplot(ar)
plt.show()

#Eliminating the C6H6(GT) for being a redundant sensor (the C6HC molecule is a Non-Methanic Hydrocarbon (NMHC)), so
#the correlation between those two sensor is exact

ar_filt.drop('C6H6(GT)', axis=1, inplace=True)

#Creating a Regression Model of the PT08.S1 sensor:

#Splitting the dataset in 80% for training and 20% for testing

from sklearn.model_selection import train_test_split

Y = ar_filt['PT08.S1(CO)'] 
X = ar_filt.drop(['PT08.S1(CO)','Date', 'Time', 'Week Day'], axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape)

from sklearn.ensemble import RandomForestRegressor

modelo_randomforest = RandomForestRegressor(n_estimators=100)

modelo_randomforest.fit(X_train, Y_train)

#Evaluating the results with the R² metric:

#Test data evaluation

from sklearn import metrics

pred_randomforest = modelo_randomforest.predict(X_test) #predicted CO concentrations

print('Random Forest Regression Model: R²={:.2f}'.format(metrics.r2_score(Y_test, pred_randomforest)))

#Comparing model predictions with the Ground Truth Test Data

aux = pd.DataFrame()

aux['Y_test'] = Y_test
aux['Predictions RandomForest_01'] = pred_randomforest

plt.figure(figsize=(15,5))
sns.lineplot(data=aux.iloc[:200,:])
plt.show()

#Extracting season information from Date column:

def season(date):
    year = str(date.year)
    seasons = {'spring': pd.date_range(start='21/03/'+year, end='20/06/'+year),
               'summer': pd.date_range(start='21/06/'+year, end='22/09/'+year),
               'autumn': pd.date_range(start='23/09/'+year, end='20/12/'+year)}
    if date in seasons['spring']:
        return 'spring'
    if date in seasons['summer']:
        return 'summer'
    if date in seasons['autumn']:
        return 'autumn'
    else:
        return 'winter'

ar_filt['Season'] = ar_filt['Date'].map(season)
ar_filt.head(10)

sns.pairplot(ar_filt, hue='Season')
plt.show()

#Creating categorical features from Season column and splitting new dataframe

Y_2 = ar_filt['PT08.S1(CO)']
X_2 = ar_filt.drop(['PT08.S1(CO)','Date', 'Time', 'Week Day'], axis=1)
X_2 = pd.get_dummies(data=X_2)

X_2_train, X_2_test, Y_2_train, Y_2_test = train_test_split(X_2, Y_2, test_size=0.2, random_state=42)
X_2.head()

modelo_randomforest_2 = RandomForestRegressor()

modelo_randomforest_2.fit(X_2_train, Y_2_train)

pred_randomforest_2 = modelo_randomforest_2.predict(X_2_test)

print('Regression Model without Seasons: R²={:.2f}'.format(metrics.r2_score(Y_test, pred_randomforest)))
print('Regression Model with Seasons: R²={:.2f}'.format(metrics.r2_score(Y_2_test, pred_randomforest_2)))

aux['Predictions RandomForest_02'] = pred_randomforest_2

plt.figure(figsize=(15,5))
sns.lineplot(data=aux.iloc[:200,:])
plt.show()

#It can be seen that the new model used the new information to react slightly better to abrupt changes
#in the predicted variable.